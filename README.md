# ADS
ADS assessment repo featuring classification &amp; NLP datasets, modeling scripts, and evaluation results.

Good morning both. Today I’ll show how we assess machine learning readiness and deliver measurable value, using two short case studies as part of the ADS program.
I’ll start with the big picture, then walk through the data and methods, the results, the risks and mitigations, and we’ll finish with impact, recommendations, and next steps. I’ll leave time for questions at the end, but feel free to interrupt me as we go on.
Right now, many decisions rely on manual, inconsistent analysis. That slows response times and makes return of investment hard to prove. 
Our mission is to quickly identify high-value opportunities, stand up accurate and explainable baselines, and demonstrate uplift against the current process so we can prioritize what goes to production.
We take a straightforward approach: rapid discovery to understand the data and business levers, build lean baselines, validate uplift with clear metrics and explainability, and plan the path to deploy with machine learning operations. 
The toolkit covers data discovery, feature engineering, AutoMachinelearning and custom models for tabular, time series, and NLP; experiment tracking; explainability and bias checks; and a model registry with monitoring. 
Delivery-wise, we could produce reusable features and pipelines, support batch and real-time scoring, provide API endpoints for dashboards and apps, and set up retraining and drift alerts. 
My own background/differentiator is a PhD-level data scientist with additional work in prompt engineering, which helps bridge advanced methods with practical stakeholder needs.
On the data: I was provided three CSV files, two tabular sets for credit risk and one NLP set of articles. 
The objectives were to assess machine learning value quickly and build baseline models. The outcomes we target are validated uplift, clear metrics, explainability, and a governance-aligned roadmap to scoring.
Overviewing the data:
Our first case study focuses on credit risk: we’re standardising decisions, cutting manual review, and enabling risk-based pricing and limits by using supervised classification (as it is a categorical target) to predict a borrower’s risk band from applicant and account features. The outcome is a consistent, auditable segmentation that speeds approvals and lifts portfolio quality. 
The second case study turns unstructured articles into early-warning signals: using an NLP pipeline, each piece is automatically assigned to a curated topic and given a sentiment score, so we can track how sentiment shifts by topic, source, and category over time. The outcome is a real-time dashboards and alerts that surface emerging themes, risks, and opportunities quickly enough to guide action. 
Let me start with credit risk model. The first thing I did was a bit of exploratory data analysis. This was to assess the completeness and missingness of the data apart from duplicates and odd values. The first question I asked was: What is the number of rows and columns in the datasets? Here we see that the train dataset has different number of columns than the test dataset. The next question was: What is the missing column(s) from the test dataset? The “test” file was missing the credit_score label, so I decided to discard it and use only the training file to create an internal train/validation split for fair evaluation. Continuing with the exploration of the training dataset, I wanted to see if the datatypes were correct and for example, we can see that some numerical columns have a data type of a string, like in the case of annual income. Next, I wanted to see the summary statistics for the columns. Here I identified odd formats such as credit history given as “15 years 11 months” and odd values such as __1000__ for amount invested monthly. 
Moreover, there seems to not be any missing data or duplicates. Looking into the target, by looking at its distribution I noted a class imbalance with the Standard band dominating. These risks over-predicting the dominant “Standard” class, seeming accurate overall but performing poorly on minority classes, thus accuracy score alone can mislead.
Looking into the numerical features correlations among them didn’t show strong multicollinearity; the highest pairwise correlation was around 0.40. So, there was no need to drop features purely on correlation grounds. That helps preserve interpretability and signal.
The pipeline for this case study was simple and robust. 
To repeat what we're trying to look here is to predict each customer's credit tier from demographics and financial data to guide underwriting decision. 
First, we have the raw data, we have the customer demographic and credit profile data. For example, the age annual income, the enhanced salary, etcetera and now what we want to do is first of all data clean so.
The first step was cleaning. Here I drop IDs and social security number for security reasons, fix types, and remove odd values, for example people older than 100 years. 
Second, I performed some feature engineering. I imputed missing values, using the median for numerical and mode for categorical features. I also did a one-hot encode for the categorical features so it’s easier for the machine to process it, and I scaled numerical features to prevent magnitude from masquerading as importance. 
Third, for the modelling: I trained a Logistic Regression baseline for speed, calibration, and interpretability, and a Random Forest to capture non-linearities and interactions. We tuned both using Randomized Search with stratified k-fold cross-validation to get good coverage quickly while providing a more robust, low-variance estimate of performance and preserving class ratios in each fold; this is a faster first pass than an exhaustive grid search and helps guard against overfitting, especially with the class imbalance.
For evaluation, I used accuracy: Overall % correct, precision: when we predict Poor, how often are we right?, recall: among truly Poor, how many did we catch?, F1: harmonic mean of precision & recall macro-F1 treats all classes equally (important when minority risk classes matter), and confusion matrices. I we prioritised: Recall on “Poor”. I want to minimise false negatives (i.e., avoid approving riskier applicants) and Macro-F1 so performance isn’t dominated by the majority “Standard” class. Another metric I could have used is ROC-AUC, which, in plain terms, measures how well the model orders applicants from riskiest to safest; I didn’t use it here because with three classes you must compute an AUC per class and then average them, different averaging choices can confuse stakeholders, and, more importantly, our business goal is catching Poor at the actual decision threshold, which recall and macro-F1 capture more directly than a ranking score.
For results logistic regression the pros are that it is fast, well-calibrated probabilities, highly interpretable coefficients. The cons: it can underfit complex interactions; sensitive to scaling so it needs standardisation and collinearity it has unstable coefficients without regularisation. Here we have the best params and the classification report. For Random Forest the pros are it captures non-linearities and interactions out-of-the-box; robust to outliers; built-in feature importance; minimal scaling needs so it needs little preprocessing. The cons: less glass-box than logistic where the inner workings are transparent so harder global explanation; slightly heavier to score therefore more CPU/latency. Here are its results.
Why not other models such as SVM, XGBoost and Neural Nets? SVMs can be slower to scale and trickier for well-calibrated probabilities without extra steps. XGBoost is excellent, but the hyperparameter space is heavier, longer tuning cycles. Neural nets on tabular data often need more data/feature work and add governance overhead. I optimised for speed, clarity, and measurability in this first pass; we can always escalate model complexity once we’ve proven value.
Confusion matrices confirmed that the Forest reduces false negatives on “Poor” without collapsing precision.
Looking at the comparison table: The Random Forest outperformed Logistic Regression on macro-F1 and maintained stronger recall on the Poor tier. That combination of good overall balance with fewer missed risky applicants makes the Random Forest the practical first scoring choice. Logistic Regression remains a valuable challenger for interpretation and as a fallback. 
Switching to the article dataset. I repeated the health checks: types, missingness, duplicates, and canonical text fields.
There was some missing data, but the data types looked right. When observing the summary statistics, I saw that for example, the URL column had one URL with a frequency of 2 that meant that there could be duplicates. Looking for duplicates I found that there were 3543, which accounted for the difference between the count and unique items in the URL column.
Plotting the timeline of the articles, there was a large spike on 2-Nov-2023. This meant that there were more articles on that day in the dataset. Searching on the internet of what global events might have happened that day I saw a couple of examples such as: UK interest, FTX scandal, the war in Gaza and the US stocks rallying with optimism. This matters because topic models can over-index on spike days; I treated that day with caution in interpretation.
To repeat what we're trying to do here is convert raw articles into actionable insights by auto-detecting topics and sentiment, enabling trend tracking and faster, data-driven decisions.
Our pipeline began as like the classification model with the raw data which in this case is a collection of articles. For example, there is one about a Nepalese earthquake.
For text preprocessing: I remove duplicates, parsed dates, made everything lowercase to normalize the words, remove punctuation/stopwords, then I tokenized the text so it’s easier for the machine to learn, and finally I stemmed the words to shorten them to their roots. I chose stemming for speed as it’s a lightweight option for large datasets for topic tagging. I didn’t choose lemmatization another way of shortening word because it is slower and more useful when grammar matters.
For Feature extraction: I chose TF-IDF to translate text into sparse numeric vectors. I choose TF-IDF because it shrinks boring common words and boosts meaningful ones, helping topic models discover clearer, more distinct topics.
In the modelling part of the pipeline for topics I used NMF (Non-negative Matrix Factorization), a parts-based method that yields additive, interpretable themes. I didn’t lead with LDA/LSA or embedding methods (e.g., BERTopic) because they’re heavier to tune, can be harder to interpret quickly, and weren’t necessary for a first, value-focused pass.
And for the sentiment detection I used VADER, a lexicon-based approach that works well on news snippets and headlines, importantly it doesn’t need labels. The con is that sarcasm/negation can trip any single-document label, so I focus on trend-level signals.
In the case of the NMF I selected 10 topics for readability and low overlap, it’s the sweet spot for a meeting: enough coverage without redundancy. The future, I’d grid-search topic counts and select via topic coherence to evaluate the best number of topics.
Looking at the top stems words they were business-centric, “share,” “stock,” “quarter,” “report,” “company”, which suggests a possible editorial focus on business coverage.
So, after fitting an NMF model with 10 components to the articles you have the words as columns and the topics as rows. Choosing a component, such as topic 0, and looking at which words have the highest values, we see that they fit a theme. So if NMF is applied to documents, then the components correspond to topics, and the NMF features reconstruct the documents from the topics.
Here are the 10 NMF topics with the 8 highest value words. Then I was able to easily label the topics for example quarterly earnings, Israel–Gaza updates, FTX trial, generative AI, and others. By what we talked before we can see that several were influenced by the 2-Nov spike. In the future we could potentially neutralise this via rolling windows or down-weighting spike days.
On sentiment, we score each article and see a distribution that skews positive overall (~70%), with ~25% negative and a smaller neutral slice. What to take from this? We can also test whether outlets with left- or right-leaning editorial stances show systematically different sentiment on certain topics. For example, pockets of concentrated negativity that may reflect editorial focus rather than event tone. Over time, sentiment largely mirrors the article-volume curve we saw earlier, slightly more positive overall, including on the spike day, but I also checked sentiment over time specifically to see if there were distinct negativity spikes at particular moments.
Some risks are data access/quality issues; underperformance or bias; integration and operational failures; security/privacy exposures; low adoption; scope creep. Constraints could be imbalanced labels, compliance obligations, budget and fixed release windows, external dependencies, and limited SME bandwidth. Some mitigations I  propose are Data contracts + automated checks; profiling for types, missingness, drift. Staged access with approvals in parallel to delivery. Start with simple baselines and track explainable metrics; escalate complexity when value justifies it. Modular APIs, CI/CD, retries/rollback. Least-privilege access, encryption, audit logging. Adoption plan with clear KPIs and tight scope control to avoid sprawl.
As a strategic recommendation maybe productionise the winning credit model first, batch scoring now, real-time endpoint next. Pipe scores into the BI layer and decisioning workflows; add guardrails: minimum confidence, human-in-the-loop on edge cases. For NLP, deploy a rolling topic/sentiment service with alerts for spikes or sentiment flips, and a simple self-serve filter by source/category. Do some govern & monitor: version every model, track performance & drift, and schedule periodic fairness checks. Then finally scale the pattern to adjacent use cases.
For next steps, in the short term, I suggest lock success measures, finalize data pipelines, and publish a minimal, trustworthy BI MVP; in the mid term, pilot with a small user group, stand up monitoring, and provide lightweight runbooks; and in the long term, go live to production, set a retraining/maintenance cadence with promotion gates, and line up the next use cases, delivering value fast, learning safely, and scaling with discipline.
